{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":161278765,"sourceType":"kernelVersion"},{"sourceId":164198301,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emdogan/945-deberta-3-base-striding-inference-tercume?scriptVersionId=166929059\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"\n\"\"\"## 🛑 Wait a second - maybe also look at the training\n- My training notebook (containing equally many emojis) is here: I would love an upvote if you use the notebook or learned something new!\n- https://www.kaggle.com/code/valentinwerner/915-deberta3base-training\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:08:20.138981Z","iopub.execute_input":"2024-03-04T07:08:20.139475Z","iopub.status.idle":"2024-03-04T07:08:20.146211Z","shell.execute_reply.started":"2024-03-04T07:08:20.139429Z","shell.execute_reply":"2024-03-04T07:08:20.145226Z"}}},{"cell_type":"markdown","source":"**Training notebook linkini eklemiş**","metadata":{}},{"cell_type":"markdown","source":"## 🏟️ Credits (because this baseline did mostly already exist when I joined)\n\n- Stride is something Raja first shared: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/473011\n- The biggest booster in performance is changing triplets to pairs in token reassembling: https://www.kaggle.com/code/nbroad/transformer-ner-baseline-lb-0-854/comments#2659393","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:purple; color:white; padding:10px; border-radius: 5px; text-align:center;\">\n  <h1>Kod blok no: 1</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"# dosya yolundan modelin yolunu bulmak için listeleme\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INFERENCE_MAX_LENGTH=1024\nSTRIDE=384\n\n# Note that training a model with stride, such as: https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b\n# will also improve performance\nmodel_path = \"/kaggle/input/915-deberta3base-training/deberta3base_1024/\"\n\n\"\"\"\n---------------------------------\n- modelde kullanılmak üzere değişkenler ve değerler belirlenmiş.\n\n+ maximum giriş uzunluğu yani \"INFERENCE_MAX_LENGTH\" 1024 karakter olarak belirlenmiş\n+ modelin üzerindeki adım büyüklüğümüz yani \"STRIDE\" değeri 384 olarak belirlenmiş\n+ \"model_path\" değişkenine deberta3 modelinin yolu atanmış\n+ not olarak stride ile çalışmanın performansa katkısı olduğu belirtilmiş\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport argparse\nfrom itertools import chain\nimport pandas as pd\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom datasets import Dataset \nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ♟️ Data Loading & Data Tokenization\n- This tokenizer is actually special, comparing to usual NLP challenges\n- inference tokenizer is a bit different than training tokenizer, because we don't have labels","metadata":{}},{"cell_type":"code","source":"def tokenize(example, tokenizer):\n    text = []\n    token_map = []\n    \n    idx = 0\n    \n    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        text.append(t)\n        token_map.extend([idx]*len(t))\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        idx += 1\n        \n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH, stride=STRIDE, return_overflowing_tokens=True)\n        \n    return {\n        **tokenized,\n        \"token_map\": token_map,\n    }\n\n\"\"\"\n---------------------------------\n- tokenize fonksiyonu oluşturulmuş, (bir önceki incelenen model ile aynı mantıkta çalışıyor) metni alıp tokenize edip bize bir \n    sözlük döndürüyor,\n\n+ fonksiyon içindeki her döngüde label \"text\" listesine eklenir, \"token_map\" değişkenine de labelin orjinal metindeki konumu atanır\n+ return olarak döndürülen sözlükte tokenized yani tokenize edilmiş metnimiz ve token_map değişkeni yani tokenlerin metindeki konumlarını \n    döndürmüş olur\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####\ndata = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [x[\"document\"] for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n})\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=2)\n\n\"\"\"\n---------------------------------\n- Test dosyası json formatında olduğu için \".load\" ile \"data\" değişkenine yüklenmiş ardından sözlük haline getirilip \"ds\" değişkenine \n    atanmış  ve \"ds\" değişkenine tokenizer uygulanmış.\n    + Yani artık test setimiz sözlük biçiminde ve tokenize edilmiş olarak \"ds\" değişkeninde.\n  \n+ önceden eğitilmiş tokenizer modelini \"tokenizer\" değişkenine atamış.\n+ map fonksiyonu ile ds değişkenindeki her değişkene tokenize uygulanmış ve kullanılcak olan işlemci çekirdeği 2 olarak belirlenmiş\n---------------------------------\n\"\"\"\n\n\"\"\"\n# datasetin ekrana yazdırılmış hali\nimport pandas as pd\nds_df = ds.to_pandas()\nprint(ds_df)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 🏋🏻‍♀️ Trainer Class based on the trained model","metadata":{}},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(model_path)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\nargs = TrainingArguments(\n    \".\", \n    per_device_eval_batch_size=1, \n    report_to=\"none\",\n)\ntrainer = Trainer(\n    model=model, \n    args=args, \n    data_collator=collator, \n    tokenizer=tokenizer,\n)\n\n\"\"\"\n---------------------------------\n- eğitimde kullanılması için model kolektör argüman değerleri ve train değerlerine atama yapılmış\n\n+ daha önce eğitilmiş model kullanılarak \"model\", \"collator\" değişkenlerine atama yapılmış\n+ eğitim argümanları \"args\" değişkenine tanımlanmış\n  + \".\" modelin eğitim sırasında kaydedilceği dizini temsil ediyor yani current location\n  + eğitimde her bir GPU veya TPU yani kullanılacak device için, kullanılcak örneklerin sayısı 1 olarak belirlenmiş\n  + eğitim sırasında günlüklerin logların nereye kaydedilceği None olarak belirlenmiş yani log kaydı tutulmayacak\n+ eğitim sırasında kullanılcak model, argüman, kollektör ve tokenizer ataması trainer nesnesi içinde atanmış\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:purple; color:white; padding:10px; border-radius: 5px; text-align:center;\">\n  <h1>Kod blok no: 2</h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"### Striding functions\n\nAs using the stride give an overlap in tokens, these have to be removed (either pick one side of the stride or average them, ...).","metadata":{}},{"cell_type":"code","source":"def backwards_map_preds(sub_predictions, max_len):\n    if max_len != 1: # nothing to map backwards if sequence is too short to be split in the first place\n        if i == 0:\n            # First sequence needs no SEP token (used to end a sequence)\n            sub_predictions = sub_predictions[:,:-1,:]\n        elif i == max_len-1:\n            # End sequence needs to CLS token + Stride tokens \n            sub_predictions = sub_predictions[:,1+STRIDE:,:] # CLS tokens + Stride tokens\n        else:\n            # Middle sequence needs to CLS token + Stride tokens + SEP token\n            sub_predictions = sub_predictions[:,1+STRIDE:-1,:]\n    return sub_predictions\n\n\"\"\"\n---------------------------------\n- Alt tahminlerin düzeltilmesi ve düzenlenmesine yardımcı olur \n\n+ alt tahmin: Büyük metin parçalarını daha küçük parçalara bölerken ve bu parçalar üzerinde tahmin yaparken oluşur\n+ SEP (Separation): metinler arasında ayrım yapmak için kullanılan belirteçtir\n+ CLS (Classification): metnin genel temsilini yakalamayı sağlar, sınıflandırma görevlerinde modelin metni sınıf veya kategoriye \n    ayırmasına yardımcı olur\n\n+ \"max_len\" 1'e eşit değilse metin parçasının uzun olduğunu ve bölünmesi gerektiği anlamına gelir ve if içine girerek çalışır\n+ i eğer 0'a eşitse yani alt dizinin başındaysak (ilk alt dize ise) bu koşula girer ve buraya eklenen belirteçlerin (SEP) tahminlerden \n    kaldırılmasını sağlar\n+ i eğer max_len-1 ise yani son alt dizi ise bu koşula girilir, eklenen belirteçlerin (CLS) tahminlerden kaldırılmasını sağlar\n+ diğer iki koşul değilse else durumuna girer ve bu da dizinin orta bloğunda olduğumuzu temsil eder bu durumda da eklenen belirteçlerin \n    (SEP, CLS) tahminlerden kaldırılması sağlanır\n---------------------------------\n\"\"\"\n\ndef backwards_map_(row_attribute, max_len):\n    # Same logics as for backwards_map_preds - except lists instead of 3darray\n    if max_len != 1:\n        if i == 0:\n            row_attribute = row_attribute[:-1]\n        elif i == max_len-1:\n            row_attribute = row_attribute[1+STRIDE:]\n        else:\n            row_attribute = row_attribute[1+STRIDE:-1]\n    return row_attribute\n\n\"\"\"\n---------------------------------\n- Tahminlerin yanı sıra attributes'ların da düzenlenmesine yardımcı olur, backwards_map_preds ile benzerdir ama Preds yani tahminler \n    yerine öznitekiklerin düzenlenmesine odaklanır\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npreds = []\nds_dict = {\n    \"document\":[],\n    \"token_map\":[],\n    \"offset_mapping\":[],\n    \"tokens\":[]\n}\n\nfor row in ds:\n    # keys that need to be re-assembled\n    row_preds = []\n    row_offset = []\n\n    for i, y in enumerate(row[\"offset_mapping\"]):\n        # create new datasset for each of of the splits per document\n        x = Dataset.from_dict({\n            \"token_type_ids\":[row[\"token_type_ids\"][i]],\n            \"input_ids\":[row[\"input_ids\"][i]],\n            \"attention_mask\":[row[\"attention_mask\"][i]],\n            \"offset_mapping\":[row[\"offset_mapping\"][i]]\n        })\n        # predict for that split\n        pred = trainer.predict(x).predictions\n        # removing the stride and additional CLS & SEP that are created\n        row_preds.append(backwards_map_preds(pred, len(row[\"offset_mapping\"])))\n        row_offset += backwards_map_(y, len(row[\"offset_mapping\"]))\n    \n    # Finalize row\n    ds_dict[\"document\"].append(row[\"document\"])\n    ds_dict[\"tokens\"].append(row[\"tokens\"])\n    ds_dict[\"token_map\"].append(row[\"token_map\"])\n    ds_dict[\"offset_mapping\"].append(row_offset)\n    \n    # Finalize prediction collection by concattenating\n    p_concat = np.concatenate(row_preds, axis = 1)\n    preds.append(p_concat)\n\"\"\"\n---------------------------------\n- Bu blokta Datasetimizdeki her bir row için döngü başlatılmış ve önceden eğitilmiş modelimiz ile tahminleme yapılmış, ardından bu \n    tahminleri düzenleyip \"preds\" listesine eklemiş, ayrıca her bir rowun giriş özellikleri ve diğer verileri \"ds_dict\" sözlüğüne eklenmiş. \n\n+ %%time kullanarak kod bloğunun çalışma süresi ölçülmüş\n+ \"preds\" ve \"ds_dict\" kod bloğunun başında empty olarak hazır hale getirilmiş\n+ döngü başlatılmış ve her bir parça için giriş bilgilerini içeren ds oluşturulmuş ve x değişkenine atanmış\n  + token türü kimliği, giriş idleri, attention mask ve offset map bilgilerini içerir\n+ trainer fonksiyonu ile bu \"x\" değişkeni kullanılarak tahminler yapılmış\n+ \"backwards_map_preds\" ve \"backwards_map_\" kullanılarak tahmin ve offsetler düzenlenmiş\n  + tahminlerin ve offset eşleşmelerinin stride yani bölen metinler için düzenlenmesi sağlanmış\n+ \"ds_dict\" sözlüğüne \"document\", \"tokens\", \"token_map\" ve \"offset_mapping\" eklenmiş\n\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = json.load(open(Path(model_path) / \"config.json\"))\nid2label = config[\"id2label\"]\n\npreds_final = []\nfor predictions in preds:\n    predictions_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis = 2).reshape(predictions.shape[0],predictions.shape[1],1)\n    predictions = predictions.argmax(-1)\n    predictions_without_O = predictions_softmax[:,:,:12].argmax(-1)\n    O_predictions = predictions_softmax[:,:,12]\n\n    threshold = 0.9\n    preds_final.append(np.where(O_predictions < threshold, predictions_without_O , predictions))\n    \n\"\"\"\n---------------------------------\n- eşik değeri %90 yani 0.9 olarak belirlenerek tahminler son hale getirilmiş ve \"preds_final\" dizisine atanır.\n\n+ \"config\" değişkenine load metodu ile önceden eğitilmiş modelin yapılandırması yüklenmiş\n+ yüklenen yapılandırma dosyasından etiketlerin dizisini yani \"id2label\" bilgilerini çekmiş ve \"id2label\" değişkenine atamış\n+ tahminler üzerinden döngü başlatılmış\n  + softmax fonksiyonu kullanılarak tahminlerin olasılıkları elde edilmiş \"predictions_softmax\" değişkenine atanmış\n  + her bir tahmin bir sınıf indisi haline getirilmiş, en yüksek olasılığa sahip sınıfın indisi alınmış ve \"predictions\" değişkenine \n    atanmış\n  + tahminlerde \"o\" etiketi içermeyen verilerin en yüksek olasılığa sahip olan etiketini belirlemiş ve \"predictions_without_O\" değişkenine\n    atamış\n  + \"o\" etiketlerinin olasılıklarını \"O_predictions\" atamış\n  + eşik değeri yani güvenilirlik oranımız 0.9 olarak belirlenmiş\n  + \"preds_final\" listesine append ile \"o\" etiketinin olasılığı 0.9'dan küçük olup olmamasına göre ihtimali en yüksek olan etiketi atamış\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:purple; color:white; padding:10px; border-radius: 5px; text-align:center;\">\n  <h1>Kod blok no: 3</h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"### Reassembling\nNote that triplets was changed to pairs to remove the FN predictions created by ignoring new triplets","metadata":{}},{"cell_type":"code","source":"ds = Dataset.from_dict(ds_dict)\npairs = []\ndocument, token, label, token_str = [], [], [], []\nfor p, token_map, offsets, tokens, doc in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n    for token_pred, (start_idx, end_idx) in zip(p[0], offsets):\n        label_pred = id2label[str(token_pred)]\n\n        if start_idx + end_idx == 0: continue\n\n        if token_map[start_idx] == -1:\n            start_idx += 1\n\n        # ignore \"\\n\\n\"\n        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n            start_idx += 1\n\n        if start_idx >= len(token_map): break\n\n        token_id = token_map[start_idx]\n\n        # ignore \"O\" predictions and whitespace preds\n        if label_pred != \"O\" and token_id != -1:\n            pair=(doc, token_id)\n\n            if pair not in pairs:\n                document.append(doc)\n                token.append(token_id)\n                label.append(label_pred)\n                token_str.append(tokens[token_id])\n                pairs.append(pair)\n                \n\"\"\"\n---------------------------------\n- tahminlerden ve \"ds\" veri kümesindeki bilgilerden yararlanarak belirli bir belirteç ile ilişkilendirilen çiftler oluşturulmuş,\n  her bir çift bir tokenin tahmin edilen etiketiyle birlikte belirtilen döküman ve belirteç kimliğini içeriyor\n\n+ Yukarıda oluşturduğumuz \"ds_dict\" sözlüğünden dataset oluşturulmuş \"ds\" değişkenine atanmış\n+ çiftleri ve etiketle ilişkilendirilmiş bilgileri depolamak için boş listeler oluşturulmuş\n+ tahminler ve diğer bilgiler ( token haritası, offsetler, tokenler, dökümanlar) üzerinde zip ile döngü oluşturulmuş\n+ döngü içine döngü oluşturularak her bir tahmin ve offsetin eşleşmesi sağlanmış\n+ \"label_pred\" değişkenine tahmin edilen sınıf etiketi is2label sözlüğünden atanmış\n+ eğer tokenin offsetlerinin toplamı 0 ise yani token belgede yer almıyorsa döngü atlanıp sonraki tokene geçilmiş\n+ tokenin başlangıcında boşluk var ise boşluk karakterini atlamak için indeks arttırılmış\n+ Başlangıç indisinden başlayarak boşluk karakterlerini atlayarak geçerli bir belirteç bulana ladar döngü devam etmiş\n  + indeks metnin uzunluğundan fazla olursa döngü kırılır\n+ \"token_id\" değişkenine başlangıç indeksine karşılık gelen token map değeri atanmış\n+ tahmin edilen label \"o\" değil ve token kimliği -1 değilse, bu durumda tahmin ve belirteç bir çift oluşturmuş ve \"pair\" değişkenine \n  atanmış\n+ if koşulu ile de oluşturulan çiftin daha önce eklenip eklenmediği kontrol edilmiş eğer eklenmemişse listeye eklenmesi sağlanmış\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 🤝 Submission hand-in","metadata":{}},{"cell_type":"code","source":"####\ndf = pd.DataFrame({\n    \"document\": document,\n    \"token\": token,\n    \"label\": label,\n    \"token_str\": token_str\n})\ndf[\"row_id\"] = list(range(len(df)))\n\n\n\"\"\"\n---------------------------------\n- oluşturulan çiftlerin dataframe hali oluşturulmuş\n---------------------------------\n\"\"\"\n\n# ilk 100 satırın ekrana yazdırılması\n# display(df.head(100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)\n\"\"\"\n---------------------------------\n- row_id, document, token, label değişkenleri csv formatına dönüştürülmüş\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}