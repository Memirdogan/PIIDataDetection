{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":161278765,"sourceType":"kernelVersion"},{"sourceId":164198301,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emdogan/945-deberta-3-base-striding-inference-tercume?scriptVersionId=166929059\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"\n\"\"\"## ğŸ›‘ Wait a second - maybe also look at the training\n- My training notebook (containing equally many emojis) is here: I would love an upvote if you use the notebook or learned something new!\n- https://www.kaggle.com/code/valentinwerner/915-deberta3base-training\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-03-04T07:08:20.138981Z","iopub.execute_input":"2024-03-04T07:08:20.139475Z","iopub.status.idle":"2024-03-04T07:08:20.146211Z","shell.execute_reply.started":"2024-03-04T07:08:20.139429Z","shell.execute_reply":"2024-03-04T07:08:20.145226Z"}}},{"cell_type":"markdown","source":"**Training notebook linkini eklemiÅŸ**","metadata":{}},{"cell_type":"markdown","source":"## ğŸŸï¸ Credits (because this baseline did mostly already exist when I joined)\n\n- Stride is something Raja first shared: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/473011\n- The biggest booster in performance is changing triplets to pairs in token reassembling: https://www.kaggle.com/code/nbroad/transformer-ner-baseline-lb-0-854/comments#2659393","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:purple; color:white; padding:10px; border-radius: 5px; text-align:center;\">\n  <h1>Kod blok no: 1</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"# dosya yolundan modelin yolunu bulmak iÃ§in listeleme\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INFERENCE_MAX_LENGTH=1024\nSTRIDE=384\n\n# Note that training a model with stride, such as: https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b\n# will also improve performance\nmodel_path = \"/kaggle/input/915-deberta3base-training/deberta3base_1024/\"\n\n\"\"\"\n---------------------------------\n- modelde kullanÄ±lmak Ã¼zere deÄŸiÅŸkenler ve deÄŸerler belirlenmiÅŸ.\n\n+ maximum giriÅŸ uzunluÄŸu yani \"INFERENCE_MAX_LENGTH\" 1024 karakter olarak belirlenmiÅŸ\n+ modelin Ã¼zerindeki adÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼mÃ¼z yani \"STRIDE\" deÄŸeri 384 olarak belirlenmiÅŸ\n+ \"model_path\" deÄŸiÅŸkenine deberta3 modelinin yolu atanmÄ±ÅŸ\n+ not olarak stride ile Ã§alÄ±ÅŸmanÄ±n performansa katkÄ±sÄ± olduÄŸu belirtilmiÅŸ\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport argparse\nfrom itertools import chain\nimport pandas as pd\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\nfrom datasets import Dataset \nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## â™Ÿï¸ Data Loading & Data Tokenization\n- This tokenizer is actually special, comparing to usual NLP challenges\n- inference tokenizer is a bit different than training tokenizer, because we don't have labels","metadata":{}},{"cell_type":"code","source":"def tokenize(example, tokenizer):\n    text = []\n    token_map = []\n    \n    idx = 0\n    \n    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n        text.append(t)\n        token_map.extend([idx]*len(t))\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        idx += 1\n        \n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH, stride=STRIDE, return_overflowing_tokens=True)\n        \n    return {\n        **tokenized,\n        \"token_map\": token_map,\n    }\n\n\"\"\"\n---------------------------------\n- tokenize fonksiyonu oluÅŸturulmuÅŸ, (bir Ã¶nceki incelenen model ile aynÄ± mantÄ±kta Ã§alÄ±ÅŸÄ±yor) metni alÄ±p tokenize edip bize bir \n    sÃ¶zlÃ¼k dÃ¶ndÃ¼rÃ¼yor,\n\n+ fonksiyon iÃ§indeki her dÃ¶ngÃ¼de label \"text\" listesine eklenir, \"token_map\" deÄŸiÅŸkenine de labelin orjinal metindeki konumu atanÄ±r\n+ return olarak dÃ¶ndÃ¼rÃ¼len sÃ¶zlÃ¼kte tokenized yani tokenize edilmiÅŸ metnimiz ve token_map deÄŸiÅŸkeni yani tokenlerin metindeki konumlarÄ±nÄ± \n    dÃ¶ndÃ¼rmÃ¼ÅŸ olur\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####\ndata = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [x[\"document\"] for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n})\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=2)\n\n\"\"\"\n---------------------------------\n- Test dosyasÄ± json formatÄ±nda olduÄŸu iÃ§in \".load\" ile \"data\" deÄŸiÅŸkenine yÃ¼klenmiÅŸ ardÄ±ndan sÃ¶zlÃ¼k haline getirilip \"ds\" deÄŸiÅŸkenine \n    atanmÄ±ÅŸ  ve \"ds\" deÄŸiÅŸkenine tokenizer uygulanmÄ±ÅŸ.\n    + Yani artÄ±k test setimiz sÃ¶zlÃ¼k biÃ§iminde ve tokenize edilmiÅŸ olarak \"ds\" deÄŸiÅŸkeninde.\n  \n+ Ã¶nceden eÄŸitilmiÅŸ tokenizer modelini \"tokenizer\" deÄŸiÅŸkenine atamÄ±ÅŸ.\n+ map fonksiyonu ile ds deÄŸiÅŸkenindeki her deÄŸiÅŸkene tokenize uygulanmÄ±ÅŸ ve kullanÄ±lcak olan iÅŸlemci Ã§ekirdeÄŸi 2 olarak belirlenmiÅŸ\n---------------------------------\n\"\"\"\n\n\"\"\"\n# datasetin ekrana yazdÄ±rÄ±lmÄ±ÅŸ hali\nimport pandas as pd\nds_df = ds.to_pandas()\nprint(ds_df)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ‹ğŸ»â€â™€ï¸ Trainer Class based on the trained model","metadata":{}},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(model_path)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\nargs = TrainingArguments(\n    \".\", \n    per_device_eval_batch_size=1, \n    report_to=\"none\",\n)\ntrainer = Trainer(\n    model=model, \n    args=args, \n    data_collator=collator, \n    tokenizer=tokenizer,\n)\n\n\"\"\"\n---------------------------------\n- eÄŸitimde kullanÄ±lmasÄ± iÃ§in model kolektÃ¶r argÃ¼man deÄŸerleri ve train deÄŸerlerine atama yapÄ±lmÄ±ÅŸ\n\n+ daha Ã¶nce eÄŸitilmiÅŸ model kullanÄ±larak \"model\", \"collator\" deÄŸiÅŸkenlerine atama yapÄ±lmÄ±ÅŸ\n+ eÄŸitim argÃ¼manlarÄ± \"args\" deÄŸiÅŸkenine tanÄ±mlanmÄ±ÅŸ\n  + \".\" modelin eÄŸitim sÄ±rasÄ±nda kaydedilceÄŸi dizini temsil ediyor yani current location\n  + eÄŸitimde her bir GPU veya TPU yani kullanÄ±lacak device iÃ§in, kullanÄ±lcak Ã¶rneklerin sayÄ±sÄ± 1 olarak belirlenmiÅŸ\n  + eÄŸitim sÄ±rasÄ±nda gÃ¼nlÃ¼klerin loglarÄ±n nereye kaydedilceÄŸi None olarak belirlenmiÅŸ yani log kaydÄ± tutulmayacak\n+ eÄŸitim sÄ±rasÄ±nda kullanÄ±lcak model, argÃ¼man, kollektÃ¶r ve tokenizer atamasÄ± trainer nesnesi iÃ§inde atanmÄ±ÅŸ\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:purple; color:white; padding:10px; border-radius: 5px; text-align:center;\">\n  <h1>Kod blok no: 2</h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"### Striding functions\n\nAs using the stride give an overlap in tokens, these have to be removed (either pick one side of the stride or average them, ...).","metadata":{}},{"cell_type":"code","source":"def backwards_map_preds(sub_predictions, max_len):\n    if max_len != 1: # nothing to map backwards if sequence is too short to be split in the first place\n        if i == 0:\n            # First sequence needs no SEP token (used to end a sequence)\n            sub_predictions = sub_predictions[:,:-1,:]\n        elif i == max_len-1:\n            # End sequence needs to CLS token + Stride tokens \n            sub_predictions = sub_predictions[:,1+STRIDE:,:] # CLS tokens + Stride tokens\n        else:\n            # Middle sequence needs to CLS token + Stride tokens + SEP token\n            sub_predictions = sub_predictions[:,1+STRIDE:-1,:]\n    return sub_predictions\n\n\"\"\"\n---------------------------------\n- Alt tahminlerin dÃ¼zeltilmesi ve dÃ¼zenlenmesine yardÄ±mcÄ± olur \n\n+ alt tahmin: BÃ¼yÃ¼k metin parÃ§alarÄ±nÄ± daha kÃ¼Ã§Ã¼k parÃ§alara bÃ¶lerken ve bu parÃ§alar Ã¼zerinde tahmin yaparken oluÅŸur\n+ SEP (Separation): metinler arasÄ±nda ayrÄ±m yapmak iÃ§in kullanÄ±lan belirteÃ§tir\n+ CLS (Classification): metnin genel temsilini yakalamayÄ± saÄŸlar, sÄ±nÄ±flandÄ±rma gÃ¶revlerinde modelin metni sÄ±nÄ±f veya kategoriye \n    ayÄ±rmasÄ±na yardÄ±mcÄ± olur\n\n+ \"max_len\" 1'e eÅŸit deÄŸilse metin parÃ§asÄ±nÄ±n uzun olduÄŸunu ve bÃ¶lÃ¼nmesi gerektiÄŸi anlamÄ±na gelir ve if iÃ§ine girerek Ã§alÄ±ÅŸÄ±r\n+ i eÄŸer 0'a eÅŸitse yani alt dizinin baÅŸÄ±ndaysak (ilk alt dize ise) bu koÅŸula girer ve buraya eklenen belirteÃ§lerin (SEP) tahminlerden \n    kaldÄ±rÄ±lmasÄ±nÄ± saÄŸlar\n+ i eÄŸer max_len-1 ise yani son alt dizi ise bu koÅŸula girilir, eklenen belirteÃ§lerin (CLS) tahminlerden kaldÄ±rÄ±lmasÄ±nÄ± saÄŸlar\n+ diÄŸer iki koÅŸul deÄŸilse else durumuna girer ve bu da dizinin orta bloÄŸunda olduÄŸumuzu temsil eder bu durumda da eklenen belirteÃ§lerin \n    (SEP, CLS) tahminlerden kaldÄ±rÄ±lmasÄ± saÄŸlanÄ±r\n---------------------------------\n\"\"\"\n\ndef backwards_map_(row_attribute, max_len):\n    # Same logics as for backwards_map_preds - except lists instead of 3darray\n    if max_len != 1:\n        if i == 0:\n            row_attribute = row_attribute[:-1]\n        elif i == max_len-1:\n            row_attribute = row_attribute[1+STRIDE:]\n        else:\n            row_attribute = row_attribute[1+STRIDE:-1]\n    return row_attribute\n\n\"\"\"\n---------------------------------\n- Tahminlerin yanÄ± sÄ±ra attributes'larÄ±n da dÃ¼zenlenmesine yardÄ±mcÄ± olur, backwards_map_preds ile benzerdir ama Preds yani tahminler \n    yerine Ã¶znitekiklerin dÃ¼zenlenmesine odaklanÄ±r\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npreds = []\nds_dict = {\n    \"document\":[],\n    \"token_map\":[],\n    \"offset_mapping\":[],\n    \"tokens\":[]\n}\n\nfor row in ds:\n    # keys that need to be re-assembled\n    row_preds = []\n    row_offset = []\n\n    for i, y in enumerate(row[\"offset_mapping\"]):\n        # create new datasset for each of of the splits per document\n        x = Dataset.from_dict({\n            \"token_type_ids\":[row[\"token_type_ids\"][i]],\n            \"input_ids\":[row[\"input_ids\"][i]],\n            \"attention_mask\":[row[\"attention_mask\"][i]],\n            \"offset_mapping\":[row[\"offset_mapping\"][i]]\n        })\n        # predict for that split\n        pred = trainer.predict(x).predictions\n        # removing the stride and additional CLS & SEP that are created\n        row_preds.append(backwards_map_preds(pred, len(row[\"offset_mapping\"])))\n        row_offset += backwards_map_(y, len(row[\"offset_mapping\"]))\n    \n    # Finalize row\n    ds_dict[\"document\"].append(row[\"document\"])\n    ds_dict[\"tokens\"].append(row[\"tokens\"])\n    ds_dict[\"token_map\"].append(row[\"token_map\"])\n    ds_dict[\"offset_mapping\"].append(row_offset)\n    \n    # Finalize prediction collection by concattenating\n    p_concat = np.concatenate(row_preds, axis = 1)\n    preds.append(p_concat)\n\"\"\"\n---------------------------------\n- Bu blokta Datasetimizdeki her bir row iÃ§in dÃ¶ngÃ¼ baÅŸlatÄ±lmÄ±ÅŸ ve Ã¶nceden eÄŸitilmiÅŸ modelimiz ile tahminleme yapÄ±lmÄ±ÅŸ, ardÄ±ndan bu \n    tahminleri dÃ¼zenleyip \"preds\" listesine eklemiÅŸ, ayrÄ±ca her bir rowun giriÅŸ Ã¶zellikleri ve diÄŸer verileri \"ds_dict\" sÃ¶zlÃ¼ÄŸÃ¼ne eklenmiÅŸ. \n\n+ %%time kullanarak kod bloÄŸunun Ã§alÄ±ÅŸma sÃ¼resi Ã¶lÃ§Ã¼lmÃ¼ÅŸ\n+ \"preds\" ve \"ds_dict\" kod bloÄŸunun baÅŸÄ±nda empty olarak hazÄ±r hale getirilmiÅŸ\n+ dÃ¶ngÃ¼ baÅŸlatÄ±lmÄ±ÅŸ ve her bir parÃ§a iÃ§in giriÅŸ bilgilerini iÃ§eren ds oluÅŸturulmuÅŸ ve x deÄŸiÅŸkenine atanmÄ±ÅŸ\n  + token tÃ¼rÃ¼ kimliÄŸi, giriÅŸ idleri, attention mask ve offset map bilgilerini iÃ§erir\n+ trainer fonksiyonu ile bu \"x\" deÄŸiÅŸkeni kullanÄ±larak tahminler yapÄ±lmÄ±ÅŸ\n+ \"backwards_map_preds\" ve \"backwards_map_\" kullanÄ±larak tahmin ve offsetler dÃ¼zenlenmiÅŸ\n  + tahminlerin ve offset eÅŸleÅŸmelerinin stride yani bÃ¶len metinler iÃ§in dÃ¼zenlenmesi saÄŸlanmÄ±ÅŸ\n+ \"ds_dict\" sÃ¶zlÃ¼ÄŸÃ¼ne \"document\", \"tokens\", \"token_map\" ve \"offset_mapping\" eklenmiÅŸ\n\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = json.load(open(Path(model_path) / \"config.json\"))\nid2label = config[\"id2label\"]\n\npreds_final = []\nfor predictions in preds:\n    predictions_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis = 2).reshape(predictions.shape[0],predictions.shape[1],1)\n    predictions = predictions.argmax(-1)\n    predictions_without_O = predictions_softmax[:,:,:12].argmax(-1)\n    O_predictions = predictions_softmax[:,:,12]\n\n    threshold = 0.9\n    preds_final.append(np.where(O_predictions < threshold, predictions_without_O , predictions))\n    \n\"\"\"\n---------------------------------\n- eÅŸik deÄŸeri %90 yani 0.9 olarak belirlenerek tahminler son hale getirilmiÅŸ ve \"preds_final\" dizisine atanÄ±r.\n\n+ \"config\" deÄŸiÅŸkenine load metodu ile Ã¶nceden eÄŸitilmiÅŸ modelin yapÄ±landÄ±rmasÄ± yÃ¼klenmiÅŸ\n+ yÃ¼klenen yapÄ±landÄ±rma dosyasÄ±ndan etiketlerin dizisini yani \"id2label\" bilgilerini Ã§ekmiÅŸ ve \"id2label\" deÄŸiÅŸkenine atamÄ±ÅŸ\n+ tahminler Ã¼zerinden dÃ¶ngÃ¼ baÅŸlatÄ±lmÄ±ÅŸ\n  + softmax fonksiyonu kullanÄ±larak tahminlerin olasÄ±lÄ±klarÄ± elde edilmiÅŸ \"predictions_softmax\" deÄŸiÅŸkenine atanmÄ±ÅŸ\n  + her bir tahmin bir sÄ±nÄ±f indisi haline getirilmiÅŸ, en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip sÄ±nÄ±fÄ±n indisi alÄ±nmÄ±ÅŸ ve \"predictions\" deÄŸiÅŸkenine \n    atanmÄ±ÅŸ\n  + tahminlerde \"o\" etiketi iÃ§ermeyen verilerin en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip olan etiketini belirlemiÅŸ ve \"predictions_without_O\" deÄŸiÅŸkenine\n    atamÄ±ÅŸ\n  + \"o\" etiketlerinin olasÄ±lÄ±klarÄ±nÄ± \"O_predictions\" atamÄ±ÅŸ\n  + eÅŸik deÄŸeri yani gÃ¼venilirlik oranÄ±mÄ±z 0.9 olarak belirlenmiÅŸ\n  + \"preds_final\" listesine append ile \"o\" etiketinin olasÄ±lÄ±ÄŸÄ± 0.9'dan kÃ¼Ã§Ã¼k olup olmamasÄ±na gÃ¶re ihtimali en yÃ¼ksek olan etiketi atamÄ±ÅŸ\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:purple; color:white; padding:10px; border-radius: 5px; text-align:center;\">\n  <h1>Kod blok no: 3</h1>\n</div>","metadata":{}},{"cell_type":"markdown","source":"### Reassembling\nNote that triplets was changed to pairs to remove the FN predictions created by ignoring new triplets","metadata":{}},{"cell_type":"code","source":"ds = Dataset.from_dict(ds_dict)\npairs = []\ndocument, token, label, token_str = [], [], [], []\nfor p, token_map, offsets, tokens, doc in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n    for token_pred, (start_idx, end_idx) in zip(p[0], offsets):\n        label_pred = id2label[str(token_pred)]\n\n        if start_idx + end_idx == 0: continue\n\n        if token_map[start_idx] == -1:\n            start_idx += 1\n\n        # ignore \"\\n\\n\"\n        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n            start_idx += 1\n\n        if start_idx >= len(token_map): break\n\n        token_id = token_map[start_idx]\n\n        # ignore \"O\" predictions and whitespace preds\n        if label_pred != \"O\" and token_id != -1:\n            pair=(doc, token_id)\n\n            if pair not in pairs:\n                document.append(doc)\n                token.append(token_id)\n                label.append(label_pred)\n                token_str.append(tokens[token_id])\n                pairs.append(pair)\n                \n\"\"\"\n---------------------------------\n- tahminlerden ve \"ds\" veri kÃ¼mesindeki bilgilerden yararlanarak belirli bir belirteÃ§ ile iliÅŸkilendirilen Ã§iftler oluÅŸturulmuÅŸ,\n  her bir Ã§ift bir tokenin tahmin edilen etiketiyle birlikte belirtilen dÃ¶kÃ¼man ve belirteÃ§ kimliÄŸini iÃ§eriyor\n\n+ YukarÄ±da oluÅŸturduÄŸumuz \"ds_dict\" sÃ¶zlÃ¼ÄŸÃ¼nden dataset oluÅŸturulmuÅŸ \"ds\" deÄŸiÅŸkenine atanmÄ±ÅŸ\n+ Ã§iftleri ve etiketle iliÅŸkilendirilmiÅŸ bilgileri depolamak iÃ§in boÅŸ listeler oluÅŸturulmuÅŸ\n+ tahminler ve diÄŸer bilgiler ( token haritasÄ±, offsetler, tokenler, dÃ¶kÃ¼manlar) Ã¼zerinde zip ile dÃ¶ngÃ¼ oluÅŸturulmuÅŸ\n+ dÃ¶ngÃ¼ iÃ§ine dÃ¶ngÃ¼ oluÅŸturularak her bir tahmin ve offsetin eÅŸleÅŸmesi saÄŸlanmÄ±ÅŸ\n+ \"label_pred\" deÄŸiÅŸkenine tahmin edilen sÄ±nÄ±f etiketi is2label sÃ¶zlÃ¼ÄŸÃ¼nden atanmÄ±ÅŸ\n+ eÄŸer tokenin offsetlerinin toplamÄ± 0 ise yani token belgede yer almÄ±yorsa dÃ¶ngÃ¼ atlanÄ±p sonraki tokene geÃ§ilmiÅŸ\n+ tokenin baÅŸlangÄ±cÄ±nda boÅŸluk var ise boÅŸluk karakterini atlamak iÃ§in indeks arttÄ±rÄ±lmÄ±ÅŸ\n+ BaÅŸlangÄ±Ã§ indisinden baÅŸlayarak boÅŸluk karakterlerini atlayarak geÃ§erli bir belirteÃ§ bulana ladar dÃ¶ngÃ¼ devam etmiÅŸ\n  + indeks metnin uzunluÄŸundan fazla olursa dÃ¶ngÃ¼ kÄ±rÄ±lÄ±r\n+ \"token_id\" deÄŸiÅŸkenine baÅŸlangÄ±Ã§ indeksine karÅŸÄ±lÄ±k gelen token map deÄŸeri atanmÄ±ÅŸ\n+ tahmin edilen label \"o\" deÄŸil ve token kimliÄŸi -1 deÄŸilse, bu durumda tahmin ve belirteÃ§ bir Ã§ift oluÅŸturmuÅŸ ve \"pair\" deÄŸiÅŸkenine \n  atanmÄ±ÅŸ\n+ if koÅŸulu ile de oluÅŸturulan Ã§iftin daha Ã¶nce eklenip eklenmediÄŸi kontrol edilmiÅŸ eÄŸer eklenmemiÅŸse listeye eklenmesi saÄŸlanmÄ±ÅŸ\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ğŸ¤ Submission hand-in","metadata":{}},{"cell_type":"code","source":"####\ndf = pd.DataFrame({\n    \"document\": document,\n    \"token\": token,\n    \"label\": label,\n    \"token_str\": token_str\n})\ndf[\"row_id\"] = list(range(len(df)))\n\n\n\"\"\"\n---------------------------------\n- oluÅŸturulan Ã§iftlerin dataframe hali oluÅŸturulmuÅŸ\n---------------------------------\n\"\"\"\n\n# ilk 100 satÄ±rÄ±n ekrana yazdÄ±rÄ±lmasÄ±\n# display(df.head(100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)\n\"\"\"\n---------------------------------\n- row_id, document, token, label deÄŸiÅŸkenleri csv formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}