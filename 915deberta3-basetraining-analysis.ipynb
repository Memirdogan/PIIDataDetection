{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":163088908,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emdogan/915-deberta3base-training-tercume?scriptVersionId=166933418\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## 🛑 Wait a second - after this you should also look at the inference notebook\n- My inference notebook (containing equally many emojis) is here:\n- https://www.kaggle.com/code/valentinwerner/893-deberta3base-inference","metadata":{}},{"cell_type":"markdown","source":"## 🏟️ Credits (because this baseline did mostly already exist when I joiend)\n\n- @Nicholas Broad published the transformer baseline which performs only marginally worse: https://www.kaggle.com/code/nbroad/transformer-ner-baseline-lb-0-854\n- @Joseph Josia published the training notebook which I basically copy pasted (which is based itself on nbroad, but yeah): https://www.kaggle.com/code/takanashihumbert/piidd-deberta-model-starter-training\n\n","metadata":{}},{"cell_type":"markdown","source":"## 💡 What I added\n- Downsampling negative samples (samples without labels, but they possible still work as examples where names should not be tagged as name)\n- Adding @moths external data: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/469493\n- Adding PJMathematicianss external data: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/470921\n- However, I used my cleaned version instead (the punctuation is flawed in the original data set at the time of this trainign): https://www.kaggle.com/code/valentinwerner/fix-punctuation-tokenization-external-dataset\n\nDoing this brought the LB score to .888 - Trained in Kaggle Notebook, no tricks or secrets.\n\n- I added emojis because that seems to be the kaggle upvote meta","metadata":{}},{"cell_type":"markdown","source":"## 📝 Config & Imports\n- 1024 max length has been working well for me. As some samples are longer, you may want to go as high as you can ","metadata":{}},{"cell_type":"code","source":"TRAINING_MODEL_PATH = \"microsoft/deberta-v3-base\"\nTRAINING_MAX_LENGTH = 1024\nOUTPUT_DIR = \"output\"\n\"\"\"\n-------------------------------------\n- Model için ayarlamalar ve import gerçekleştirilmiş.\n\n+ TRAINING_MODEL_PATH: eğitimde kullanılacak modelin adı bu değişkene atanmış, DeBERTa modeli kullanılmış.\n+ TRAINING_MAX_LENGTH: eğitimde kullanılacak maksimum giriş uzunluğunu belirlenmiş.\n+ OUTPUT_DIR: eğitim sırasında oluşturulan çıktı dosyalarının kaydedileceği dizin belirtilmiş, çıktıların \"output\" adlı bir dizine \n  kaydedileceği belirlenmiş.\n-------------------------------------\n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval evaluate -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport argparse\nfrom itertools import chain\nfrom functools import partial\n\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 🗺️ Data Selection and Label Mapping\n- As mentioned before, I additionaly use the moth dataset","metadata":{}},{"cell_type":"code","source":"data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"))\n\n# downsampling of negative examples\np=[] # positive samples (contain relevant labels)\nn=[] # negative samples (presumably contain entities that are possibly wrongly classified as entity)\nfor d in data:\n    if any(np.array(d[\"labels\"]) != \"O\"): p.append(d)\n    else: n.append(d)\n        \n# orjinal örnek sayısı\nprint(\"original datapoints: \", len(data))\n\n# ek veri setinin örnek sayısı\nexternal = json.load(open(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/pii_dataset_fixed.json\"))\nprint(\"external datapoints: \", len(external))\n\n# ek veri setinin örnek sayısı\nmoredata = json.load(open(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/moredata_dataset_fixed.json\"))\nprint(\"moredata datapoints: \", len(moredata))\n\n# tüm veri setinin örnek sayısı\ndata = moredata+external+p+n[:len(n)//3]\nprint(\"combined: \", len(data))\n\"\"\"\n--------------------------\n- ekstra datasetler import edilmiş ve örnek sayıları ekrana bastırılmış\n\n+ \"data\" değişkenine yarışmanın kendi train seti import edilmiş\n+ Pozitif ve negatif değerler \"p\" ve \"n\" listelerine atanıcak\n  + data değişkeni üzerinde döngü başlatılmış\n  + \"O\" etiketine sahip olmayan etiketler pozitif yani \"p\" listedine eklenmiş\n  + \"O\" etiketleri ise negatif yani \"n\" değişkenine atanmış\n  + döngü bittikten sonra orjinal data sayısını yani örnek sayısını ekrana printlemiş\n+ dışardan dataset elde edilip \"external\" değişkenine import edilmiş\n  + ekrana bu ds'in kaç örnek içerdiği yazdırılmış\n+ \"moredata\" değişkenine dışardan başka ds yüklenmiş\n  + ekrana kaç örnek içerdiği yazdırılmış\n+ \"data\" değişkenine dışardan import edilen ds'ler dahil olmak üzere \"p\" ve \"n\" listeleri de dahil edilerek birleştirme yapılmış\n  + combined adı altında tüm verilerin örnek sayıları ekrana yazdırılmış\n--------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####\nall_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\nlabel2id = {l: i for i,l in enumerate(all_labels)}\nid2label = {v:k for k,v in label2id.items()}\n\ntarget = [\n    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n]\n\n# print(id2label)\n\n\"\"\"\n----------------------------\n- ds'lerimiz ile önce tüm label'Ların listesi oluşturulmuş, sonradında her bir etiketin bir indeksle eşleştirilmiş hali olan label2id ve \n  indekslerin etiketlere eşleştirilmiş hali olan id2label sözlüklerini oluşturulmuş.\n\n+ tüm örnekleri içeren \"data\" değişkenini döngüye alarak chain fonksiyonu ile labels etiketleri tek liste haline getirilmiş, \"all_labels\" \n  listesine atanmış\n  + set fonksiyonu ile listedeki labels'lar uniq hale getirilmiş yani her bir etiket listeye 1 defa eklenmiş\n  + list fonksiyonu ile oluşturulan set bir liste haline getirilmiş\n  + sorted fonksiyonu ile bu liste alfabetik olarak sıralanmış\n+ \"all_labels\" listesinden enumerate ile \"label2id\" oluşturulmuş\n+ \"label2id\" kullanılarak tam tersi yani id2label oluşturulmuş\n+ target adında bir liste oluşturulmuş ve bu listede aradığımız etiketler sıralanmış\n----------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ♟️ Data Tokenization\n- This tokenizer is actually special, comparing to usual NLP challenges","metadata":{}},{"cell_type":"code","source":"def tokenize(example, tokenizer, label2id, max_length):\n\n    # rebuild text from tokens\n    text = []\n    labels = []\n\n    for t, l, ws in zip(\n        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n    ):\n        text.append(t)\n        labels.extend([l] * len(t))\n\n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n\n    \"\"\"\n    -------------------------------------------------------\n        + \"text\" ve \"labels\" listeleri oluşturulmuş\n        + \"example\" sözlüğünden \"text\", \"labels\" ve \"trailing_whitespace\" değişkenleri ile döngü oluşturulmuş\n        + her bir token \"text\" listesine eklenmiş\n        + tokenin uzunluğu kadar aynı etiket birden çok kez labels'a eklenmiş çünkü her bir karakter için aynı etiket geçerli.\n        + eğer tokenin ardından boşluk karakteri varsa, boşluk karakteri text listesine eklenmiş ve labels listesine de \"o\" etiketi \n          eklenmiş\n    -------------------------------------------------------\n    \"\"\"\n\n    # actual tokenization\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n\n    labels = np.array(labels)\n\n    text = \"\".join(text)\n    token_labels = []\n    \"\"\"\n    + tokenizer çağırılarak metin tokenize edilmiş ve \"tokenized\" değişkenine atanmış\n    + \"\".join(text) ile metindeki tokenler birleştirilip tek dize hale getirilmiş\n    + return_offsets_mapping=True tokenin başlangıç ve bitiş konumlarını takip eden offsetlerin return edilmesi sağlanmış\n    + uzunluk olarak daha önce oluşturduğumuz \"max_length\" değişkeni kullanılmış\n    + \"labels\" değişkenine etiketler numpy arry olarak atanmış\n    + text değişkenine tüm tokenler birleştirilerek tekrardan atama yapılmış\n    + \"token_labels\" adında boş liste oluşturulmuş\n    \"\"\"\n    for start_idx, end_idx in tokenized.offset_mapping:\n        # CLS token\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n\n        token_labels.append(label2id[labels[start_idx]])\n\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length}\n\n\"\"\"\n-----------------------------\n- tokenized fonksiyonu oluşturulmuş metin, tokenizer, label2id ve uzunluk bilgilerini alarak metni tokenize etmemizi sağlamış, \n  return olarak tokenized metni, etiketleri ve uzunluklarını sözlük biçiminde döndürmüş.\n  \n\n+ tokenin offset maplerini kullanarak etiketlerini belirlememizi sağlayan bir döngü başlatılmış\n  + \"start_idx\" ve \"end_idx\" tokenin orjinal metindeki başlangıç ve bitiş konumlarını ifade ediyor\n  + if şartında tokenin CLS belirteçi olup olmadığını kontrol edilmiş\n  + eğer token CSL belirteçi ise tokenin etiketi other \"O\" olarak belirlenir\n+ eğer token boşluk ile başlamışsa başlangıç indeksine +1 eklenmiş\n+ indeksteki tokenin etiketi label2id sözlüğünden alınarak token_labels listesine eklenmiş\n+ tokenized sözlüğü içindeki input_ids öğesinin uzunluğu hesaplanmış, tokenleştirilmiş metindeki toplam token sayısı \"length\" değişkenine \n  atanmış.\n+ return olarak sözlük döndürülmüş\n  + tokenized sözlüğündeki tüm ögeler kapsanmış\n  + labels yani token etiketleri döndürülmüş\n  + lenght yani metindeki toplam token sayısı döndürülmüş\n-----------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)\n\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [str(x[\"document\"]) for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n    \"provided_labels\": [x[\"labels\"] for x in data],\n})\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": TRAINING_MAX_LENGTH}, num_proc=3)\n# ds = ds.class_encode_column(\"group\")\n\n\"\"\"\n--------------------------------------------\n- modelden hazır tokenizer oluşturulmuş, ds oluşturulup bunun üzerinde tokenizer uygulanmış\n\n+ \"tokenizer\" değişkenine daha önce eğitilmiş modeldeki tokenizer atanmış\n+ sözlükten bir ds oluşturulmuş ve \"ds\" değişkenine atanmış\n  + \"full_text\", \"document\", \"tokens\", \"trailing_whitespace\", \"provided_labels\" değişkenlerini içeriyor\n  + her bir değişken için önceden oluşturduğumuz \"data\" değişkenine döngü oluşturulmuş\n+ map fonksiyonu ile ds değişkenine tokenizasyon uygulanmış\n  + kullanılacak işlemci çekirdeği 3 olarak belirlenmiş\n--------------------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = ds[0]\n\nfor t,l in zip(x[\"tokens\"], x[\"provided_labels\"]):\n    if l != \"O\":\n        print((t,l))\n\nprint(\"*\"*100)\n\nfor t, l in zip(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]), x[\"labels\"]):\n    if id2label[l] != \"O\":\n        print((t,id2label[l]))\n        \n\"\"\"\n---------------------------------------\n- ds içindeki token ve etiketleri çift halinde \"o\" değilse ekrana bastırıyor ardından dsdeki tokenleri input idlere dönüştürüyor, \n  karşılık gelen etiketleri yine çift halde olarak \"o\" değilse ekrana yazdırıyor.\n\n+ ilk döngü veri kümesindeki öğenin tokenlerini ve sağlanan etiketlerini (provided_labels) alır. Her bir token ve etiket çifti için, \neğer etiket \"O\" değilse tokeni ve etiketi yazdırır.\n+ ikinci döngü veri kümesindeki öğenin tokenlerini modelin giriş kimliklerine (input_ids) dönüştürür. Daha sonra, bu giriş kimliklerine \n  karşılık gelen etiketleri (labels) alır. Her bir token ve etiket çifti için, eğer etiket \"O\" değilse, tokeni ve etiketi yazdırır. \n  +Burada id2label sözlüğü kullanılarak etiketlerin anlamlı hale dönüştürülmesi sağlanır.\n---------------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 🧮 Competition metrics\n- Note that we are not using the normal F1 score.\n- Although it is early in the competition, there are plenty of discsussions already explaining this:\n- e.g., here: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/470024","metadata":{}},{"cell_type":"code","source":"from seqeval.metrics import recall_score, precision_score\nfrom seqeval.metrics import classification_report\nfrom seqeval.metrics import f1_score\n\ndef compute_metrics(p, all_labels):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    recall = recall_score(true_labels, true_predictions)\n    precision = precision_score(true_labels, true_predictions)\n    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n    \n    results = {\n        'recall': recall,\n        'precision': precision,\n        'f1': f1_score\n    }\n    return results\n\"\"\"\n-------------------------------------------\n- modelin performansını ölçmek ve değerlendirmek için Recall, Precision ve F1 skoru hesaplanmış \"results\" sözlüğüne atanıp return edilmiş\n\n+ modelden aldığımız sonuçların softmax ile en yüksek olasılıklı olanını bulup her bir etiket için o tokenin \n  en yüksek ihtimalli etiketini seçmiş ve dizi oluşturmuş bunu da predictions değişkenine atamış\n+ her bir etiket için tahmin ve gerçek etiketler dolaştırılarak döngü oluşturulmuş,\n  + tahmin etiketler prediction değişkenine\n  + gerçek etiketler label etiketine atanır\n  + bu tahmin ve etiketler \"true_predictions\" ve \"true_labels\" değişkenlerine atanmış\n+ hazır kütüphaneler yardımı ile Recall, Precision ve f1 hesaplanmış ve değişkenlere atanmış\n+ sonuçlar \"result\" adlı sözlükte birleştirilip return edilmiş\n-------------------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(\n    TRAINING_MODEL_PATH,\n    num_labels=len(all_labels),\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True\n)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n\n\"\"\"\n---------------------------------\n- önceden eğitilmiş model ve collector için \"model\" ve \"collator\" değişkenine atama yapılmış \n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I decided to uses no eval\n# final_ds = ds.train_test_split(test_size=0.2, seed=42) # cannot use stratify_by_column='group'\n# final_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 🏋🏻‍♀️ Training\n- I actually do not use an eval set for submission to train on all data\n- Values are not really tuned and go by gut feeling, as this is my first iteration / baseline","metadata":{}},{"cell_type":"code","source":"# I actually chose to not use any validation set. This is only for the model I use for submission.\nargs = TrainingArguments(\n    output_dir=OUTPUT_DIR, \n    fp16=True,\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    report_to=\"none\",\n    evaluation_strategy=\"no\",\n    do_eval=False,\n    save_total_limit=1,\n    logging_steps=20,\n    lr_scheduler_type='cosine',\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    warmup_ratio=0.1,\n    weight_decay=0.01\n)\n\ntrainer = Trainer(\n    model=model, \n    args=args, \n    train_dataset=ds,\n    data_collator=collator, \n    tokenizer=tokenizer,\n    compute_metrics=partial(compute_metrics, all_labels=all_labels),\n)\n\n\"\"\"\n----------------------------------\n- model için argüman ve değişken ataması yapılmış\n\n+ modelin kaydedilceği yer OUTPUT_DIR olarak belirlenmiş\n+ eğitim sırasında yarı hassas hesaplama devre dışı bırakılmış\n  + fp16 hesaplama yapılırken daha az bellek kullanımı ve daha hızlı işlem yapılmasını sağlayan bir tekniktir\n+ Eğitimde verilerin 3 tur eğitilceği belirtilmiş, Her bir tur, tüm eğitim verilerinin bir kez model tarafından geçirilmesini ifade eder.\n+ eğitim sırasındaki raporların gönderilmesi istenmemiş\n+ eğitim sırasında değerlendirme yapılmaması adına ayarlar yapılmış\n+ en iyi modelin belirlenmesinde kullanılcak olan metrik f1 olarak belirlenmiş\n+ trainer sınıfındaki değişkenler için yollar tek tek atanmış\n----------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.train()\n\"\"\"\n---------------------------------\n- eğitim süresini ölçmek için time komutu kullanılarak eğitim başlatılmış.\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 💾 Save models\n- You can click on \"Save version\" (top right) and \"Save & Run All (Commit)\"\n- Then you can use this notebook as input for your inference notebook","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"deberta3base_1024\")\ntokenizer.save_pretrained(\"deberta3base_1024\")\n\n\"\"\"\n-----------------------------------------\n- trainer modeli ve tokenizer \"deberta3base_1024\" dizinine kaydedilmiş\n-----------------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}