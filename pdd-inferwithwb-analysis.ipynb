{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7590021,"sourceType":"datasetVersion","datasetId":4418096}],"dockerImageVersionId":30648,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emdogan/pii-data-detection-infer-with-w-b-tercume?scriptVersionId=166832873\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# W&B inference code for PII Data Detection competition\n\nAs presented in the [live training session](https://www.youtube.com/watch?v=w4ZDwiSXMK0).\n\nThe model being inferred here is the one we trained live during the session above!\n\nTraining code: https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:purple; color:white; padding:10px; border-radius: 5px; text-align:center;\">\n  <h1>Kod blok no: 1</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"model_path = '/kaggle/input/pii008'\ninference_max_length = 768\nmax_length = 400\ndoc_stride = 300\nthreshold=0.9\n\n\"\"\"\n---------------------------------\n- modelin yolu belirlenmiş ve gerekli değerler atanmış\n\n+ model_path değişkenine kullanılacak olan modelin yolu atanmış. \n+ Tahmin yapılırkenki girdi uzunluğu inference_max_length değişkenine 768 olarak girilmiş\n    + (768 genellikle BERT gibi modellerin girdi uzunluğuna karşılık gelir)\n+ Çıktı uzunluğuna da max_length değişkeni üzerinden 400 olarak atama yapılmış.\n+ büyük metinlerde çalışılırken belgeleri küçük metinlere parçalamak gerekebilir, doc_stride\n    değişkeni ile bu belgelerin parçalara bölme değeri 300 olarak seçilmiş.\n+ Tahminlerin ne kadar güvenilir olması gerektiğini belirlemek için kabul edilebilir \n  güvenilirlik seviyesi belirlenmiş ve burada 0.9 olarak ayarlanmış, yani tahminlerin \n  %90 veya daha fazlasının güvenilir olması gerektiği belirtilmiş.\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification, AutoTokenizer\n\nmodel = AutoModelForTokenClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n\n\"\"\"\n---------------------------------\n- model tokenizer ve collator değişkenlerine model üzerinden atama yapılmış\n\n+ Önceden eğitilmiş bir model, model değişkenine atanıyor.\n+ tokenizer değişkenine önceden eğitilmiş bir modelden tokenizer ataması yapılıyor.\n+ Eğitim sırasında modelin kullancağı veriye uygun bir şekilde veriyi hazırlamak için\n  veri koleksiyon nesnesi oluşturuluyor ve belirtilen tokenizer'ı gerekirse belirli \n  bir diziye doldurmak için kullanılan sayıyı 16 olarak alıyor.\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\n\ntrain = json.load(open(\"../input/pii-detection-removal-from-educational-data/test.json\"))\ndf = pd.DataFrame(train)\n\ndef add_token_indices(doc_tokens):\n    token_indices = list(range(len(doc_tokens)))\n    return token_indices\n\ndf['token_indices'] = df['tokens'].apply(add_token_indices)\n\n\"\"\"\n---------------------------------\n- dataset yüklenmiş ve tokenlerin indisi oluşturulup eksütun olarak eklenmiş\n\n+ df değişkenine test dosyası dataframe olarak atanıyor.\n+ add_token_indices fonksiyonu belge için token dizinlerini yani indekslerini oluşturuyor ve \n   token_indices değişkenine atıyor.\n+ dataframe'deki her token, fonksiyonun uygulanması ile indeks numarası alıyor ve\n   dataframe'deki 'token_indices' sütununa bu indeks numaraları ile atanıyor.\n---------------------------------\n\"\"\"\n\n# token indices eklenmiş df çıktısı:\n# df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rebuild_text(tokens, trailing_whitespace):\n    text = ''\n    for token, ws in zip(tokens, trailing_whitespace):\n        ws = \" \" if ws == True else \"\"\n        text += token + ws\n    return text\n\n\"\"\"\n# rebuild edilen text'in çıktısı\nfor index, row in df.iterrows():\n    text = rebuild_text(row['tokens'], row['trailing_whitespace'])\n    print(text)\n\"\"\"\n\n\"\"\"\n---------------------------------\n- rebuild_text fonksiyonu metindeki tokenleri, metindeki sırası ve aralarında boşluk olcak \n  şekilde text değişkenine atıyor ve bunu return ediyor.\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_rows(df, max_length, doc_stride):\n    new_df = []\n    for _, row in df.iterrows():\n        tokens = row['tokens']\n        if len(tokens) > max_length:\n            start = 0\n            while start < len(tokens):\n                remaining_tokens = len(tokens) - start\n                if remaining_tokens < max_length and start != 0:\n                    # Adjust start for the last window to ensure it has max_length tokens\n                    start = max(0, len(tokens) - max_length)\n                end = min(start + max_length, len(tokens))\n                new_row = {}\n                new_row['document'] = row['document']\n                new_row['tokens'] = tokens[start:end]\n                new_row['trailing_whitespace'] = row['trailing_whitespace'][start:end]\n                new_row['token_indices'] = list(range(start, end))\n                new_row['full_text'] = rebuild_text(new_row['tokens'], new_row['trailing_whitespace'])\n                new_df.append(new_row)\n                if remaining_tokens >= max_length:\n                    start += doc_stride\n                else:\n                    # Break the loop if we've adjusted for the last window\n                    break\n        else:\n            new_row = {\n                'document': row['document'], \n                'tokens': row['tokens'], \n                'trailing_whitespace': row['trailing_whitespace'], \n                'token_indices': row['token_indices'], \n                'full_text': row['full_text']\n            }\n            new_df.append(new_row)\n    return pd.DataFrame(new_df)\n\nstride_df = split_rows(df, max_length, doc_stride)\n\n# bölünmüş metinden oluşturulan df çıktısı:\n# print(stride_df)\n\n\"\"\"\n---------------------------------\n- \"split_rows\" belgeleri daha küçük parçalara bölmeyi sağlıyor ve \"max_length\" yani\n  parçaların max uzunluğunu ve \"doc_stride\" belgelerin nasıl bölüneceğini parametre olarak alıyor\n\n+ boş bir \"new_df\" oluşturulur, bu değişken belge parçalarını içericek.\n+ döngü ile df'deki tüm belgeleri işlemeye başlar, tüm tokenler işlenene kadar devam eder\n+ token sayısı maksimum uzunluktan büyükse belgeleri daha küçük parçalara böler\n+ belgenin son kısmının maximum uzunluktan kısa olup olmadığını kontrol ediyor, eğer kısaysa son kısmı maximum uzunluğa \n    sahip olacak şekilde düzenler böylelikle tüm parçalar birbirleri ile aynı uzunluğa eşit olur  \n+ ardından \"new_row\" değişkeni oluşturuyor ve orjinal belgedeki \"document\", \"tokens\", \"trailling_whitespaces\", \"token_indices\" \n    ve \"full_text\" özellikleri bu değişkene kopyalanıyor ardından \"new_df\" adlı df'e ekliyor\n+ return değeri olarak oluşturulan bu yeni dataframe yani \"new_df\" return ediliyor\n+ oluşturduğumuz \"split_rows\" fonksiyonu çağırılıyor ve sonucu \"stride_df\" değişkenine atanıyor\n---------------------------------\n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:purple; color:white; padding:10px; border-radius: 5px; text-align:center;\">\n  <h1>Kod blok no: 2</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom datasets import Dataset\n\ndef tokenize(example, tokenizer, label2id, max_length):\n\n    # rebuild text from tokens\n    text = []\n    token_map = []\n    labels = []\n    \n    idx = 0\n\n    for t, ws in zip(\n        example[\"tokens\"], example[\"trailing_whitespace\"]\n    ):\n        text.append(t)\n        token_map.extend([idx]*len(t))\n\n        if ws:\n            text.append(\" \")\n            token_map.append(-1)\n            \n        idx += 1\n\n    # actual tokenization\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n    text = \"\".join(text)\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"length\": length, \"token_map\": token_map,}\n\n\"\"\"\n---------------------------------\n- fonksiyon metni tokenize etmemizi sağlar ve tokenize edilmiş metnin uzunluğunu tutar, tokenize edilmiş metinle birlikte ek \n    bilgiler içeren bir sözlük döndürür.\n  \n+ \"text\", \"token_map\", \"labels\" adında boş listeler oluşturuluyor sırasıyla tokenlerin metin karşılıklarını, tokenlerin \n    belge içindeki konumlarını ve etiketleri saklar.\n+ for döngüsü başlatılır ve bu döngüde tokenler ile (\"tokens\") arasındaki boşluklar (\"trailing_whitespace\") birlikte \n    döndürülür her döngüde, her bir token \"text\" listesine eklenir ve tokenin konumu da \"token_map\" listesine eklenir.\n+ eğer tokenin boşluğu varsa boşluk karakteri de \"text\" listesine eklenir ve \"token_map\" listesine bu sebeple \"-1\" eklenir\n+ #actual tokenization kısmında \"text\" listesindeki tüm elemanlar birleştirilerek metin dizisi oluşturuluyor ve bu dizi tokenizere\n    iletilerek tokenize edilmiş çıktı alır ve \"return_offsets_mapping=True\" ayarı ile tokenlerin orjinal metindeki konumlarının da \n    döndürülmesini ister. tokenize edilmiş çıktı tokenized değişkenine atanır.\n+ daha sonrasında \"text\" değişkenine tokenize edilmiş metin atanır, \"length\" değişkenine tokenize edilmiş metnin uzunluğu atanır\n+ Return olarak çıktıya ek bilgiler eklenerek sözlük oluşturulmuş, tokenized, uzunluk ve \"token_map\" return edilir\n---------------------------------\n\"\"\"\n\ndef create_dataset(data, tokenizer, max_length, label2id):\n    ds = Dataset.from_dict({\n        \"full_text\": data.full_text.tolist(),\n        \"document\": data.document.tolist(),\n        \"tokens\": data.tokens.tolist(),\n        \"trailing_whitespace\": data.trailing_whitespace.tolist(),\n        \"token_indices\": data.token_indices.tolist(),\n    })\n    ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": max_length}, num_proc=3)\n    return ds\n\n\"\"\"\n---------------------------------\n- bu fonksiyon sözlükten (dictionary) veri kümesi oluşturmamıza yarar\n\n+ her bir key, bir öznitelik yani attiribute olarak kabul edilir\n+ fonksiyonda oluşturulan ds sözlüğüne tokenize fonksiyonu ile tokenizasyon işlemi yapılır \n+ son olarak da oluşturulan tokenize edilmiş data return edilir\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2label = {0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\nlabel2id = {v:k for k,v in id2label.items()}\n\"\"\"\n---------------------------------\n- etiketleri id'lerine id'lerin de etiketlere karşılık geldiği iki sözlük oluşturulmuş\n\n+ ilk satırda idlere karşılık gelen etiketlerin sözlüğü oluşturulmuş örneğin 0 idli etiket\n    \"B-EMAIL\"e karşılık geliyor ve bunlar \"id2label\" değişkeninde tutuluyor.\n+ ikinci satırda \"id2label\" değişkenindeki key ve values değerleri ters çevirilerek etiketlere\n    karşılık gelen idlerin olduğu bir sözlük oluşturulmuş ve \"label2id\" değişkenine atanmış\n---------------------------------\n\"\"\"\n\n\"\"\"\n# sözlüklerin çıktısı: \nprint(\"id2label=>\",id2label)\nprint(\"-------------------------------------------\")\nprint(\"label2id=>\",label2id)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_ds = create_dataset(stride_df, tokenizer, inference_max_length, label2id)\n\"\"\"\n---------------------------------\n- yukarıda oluşturduğumuz create_dataset fonksiyonu çağırılarak veri kümesi oluşturuluyor\n\n+ \"label2id\" değişkeni de parametre olarak gönderiliyor yani etiketlerin idlerinden yararlanılarak sözlük oluşturuluyor\n---------------------------------\n\"\"\"\n\n\"\"\"\n# oluşturulan veri kümesinin df olarak ekrana basılmış hali:\nimport pandas as pd\nvalid_df = valid_ds.to_pandas()\nprint(valid_df)\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model, \n    data_collator=collator, \n    tokenizer=tokenizer,\n)\n\"\"\"\n---------------------------------\n- transformers kütüphanesinden trainer kullanılarak modelin doğrulama veri kümesi üzerinde tahmin yapması sağlanmış.\n  \n+ trainer için atama yapılırken kullanılan \"model\", doğrulama işlemi için kullanılacak modeldir\n+ \"data_collator\" veri kümesini modelin girişine uygun hale getiren kollektör\n+ \"tokenizer\" ise metini tokenize etmek için kullanılır\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = trainer.predict(valid_ds)\n\"\"\"\n---------------------------------\n- oluşturulan trainer nesnesi kullanılarak veri kümemiz üzerinde tahmin yapılması sağlanmış\n\n+ oluşan tahminleri \"preds\" değişkenine atamış\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color:purple; color:white; padding:10px; border-radius: 5px; text-align:center;\">\n  <h1>Kod blok no: 3</h1>\n</div>","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/valentinwerner/915-deberta3base-inference?scriptVersionId=161126788\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.core.display import HTML\nimport wandb\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom tqdm.auto import tqdm\nimport spacy\nfrom spacy.tokens import Span\nfrom spacy import displacy\n\ndef parse_predictions(predictions, id2label, ds, threshold=0.9):\n    \n    pred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis = 2).reshape(predictions.shape[0],predictions.shape[1],1)\n    preds = predictions.argmax(-1)\n    preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n    O_preds = pred_softmax[:,:,12]\n    preds_final = np.where(O_preds < threshold, preds_without_O , preds)\n    \"\"\"\n    ---------------------------------\n    - bu fonksiyon oluşturup \"preds\" değişkenine atadığımız tahminleri alır ve tahminlerden çıkan varlıkları işler\n      modelin çıkardığı tahminler, modelimizdeki eşik değerinden düşük (threshold değişkeni yani 0.9) olan \"O\" etiketlerini \n      diğer en olası etiketler ile değiştiriyor.\n\n    + Eğer bir tahminin olasılığı belirli bir eşik değerinden düşükse (threshold değişkeni yani 0.9) ve bu tahmin \"O\" etiketine sahipse, \n      bu tahmin diğer en olası etiketlerle değiştirilir. Bu şekilde, modelin anlamlı varlıkları tespit etme yeteneği artırılır ve modelin doğruluğu iyileştirilir.\n    + fonksiyon sonrası elde edilen df modelin tahminlerini ve tahminlere ilişkin varlıkları içerir\n    + \"pred_softmax\" değişkeni modelin tahmin ettiği etiket olasılıklarını softmax fonksiyonu ile normalize etmiş\n    + \"preds\" değişkenine her bir tahmin için en yüksek olasılığa sahip etiketin indeksi belirlenmiş\n    + \"preds_without_0\" değişkenine \"o\" etiketi dışındaki en yüksek olasılığa sahip etiketin indeksi belirlenmiş\n    + \"O_preds\" değişkenine \"O\" etiketine ait olasılıklar belirlenmiş\n    + \"preds_final\" değişkeninde \"O\" etiketine ait olasılıkların eşik değerimizden küçük olup olmadığına bakılmış\n        eğer küçükse \"O\" etiketi yerine \"O\" olmayan en yüksek olasılığa sahip etiketin indeksi kullanılmış ve tahminler elde edilmiş\n    --------------\n    \"\"\"\n    triplets = []\n    row, document, token, label, token_str = [], [], [], [], []\n    for i, (p, token_map, offsets, tokens, doc, indices) in enumerate(zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], ds[\"token_indices\"])):\n\n        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n            label_pred = id2label[token_pred]\n\n            if start_idx + end_idx == 0: continue\n\n            if token_map[start_idx] == -1:\n                start_idx += 1\n\n            # ignore \"\\n\\n\"\n            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n                start_idx += 1\n            \"\"\"\n            --------------\n            + \"triplets\" adında, sonuçların depolanması için boş liste oluşturulmuş\n            + for döngüsü ve zip fonksiyonu ile son tahminler yani \"preds_final\" \"token_map\" \"offset_mapping\" \"tokens\" \"document\" \n                \"token_indices\" verilerinde döngü başlatılmış\n            + iç içe döngü oluşturulmuş, iç döngü her bir tokeni ve offsetlerini alıp her bir token ve tahmin edilen etiketini işlemek için kullanılmış\n            + tahmin edilen etiketin indeksi id2label kullanılarak \"label_pred\" değişkenine atanmış\n            + eğer tokenin offsetlerinin toplamı 0 ise yani token belgede yer almıyorsa döngü atlanıp sonraki tokene geçilmiş\n            + tokenin başlangıcında boşluk var ise boşluk karakterini atlamak için start_idx arttırılmış\n            + while döngüsü ile boşluk karakterleri atlanarak, metin belgesindeki tokenin başlangıç offseti belirlenmiş\n                başlangıç indeksi belirli bir boşluk karakteri değilse veya metin belgesinin sonunda değilse arttırılmış.\n            --------------\n            \"\"\"\n    \n            if start_idx >= len(token_map): break\n\n            original_token_id = token_map[start_idx]\n            token_id = indices[original_token_id]\n\n            # ignore \"O\" predictions and whitespace preds\n            if label_pred != \"O\" and token_id != -1:\n                triplet = (label_pred, token_id, tokens[original_token_id])\n\n                if triplet not in triplets:\n                    row.append(i)\n                    document.append(doc)\n                    token.append(token_id)\n                    label.append(label_pred)\n                    token_str.append(tokens[original_token_id])\n                    triplets.append(triplet)\n    \"\"\"\n    ----------------\n    + koşul ile başlangıç indeksi metin belgesinin sonuna ulaştıysa döngü sonlandırılmış\n    + \"original_token_id\" değişkenine tokenin indisi belirlenmiş\n    + \"token_id\" değişkenine tokenin indeksi atanmış\n    + if ile eğer tahmin edilen etiket \"O\" değilse ve tokenin indeksi -1 değilse yani boşluk karakteri değilse\n        etiket, tokenin indeksi, orjinal metin belgesindeki token  \"triplet\" değişkenine atanır\n    + \"triplet\" değişkenine atanan değer daha önce oluşturduğumuz \"triplets\" değişkeninde değilse listeye eklenir\n    + sonuç olarak triplets listesi, metin belgesindeki her bir token için tahmin edilen etiketleri ve bu etiketlere \n        karşılık gelen tokenlerin indislerini içerir. Ancak, bu listeye sadece \"O\" olmayan etiketler ve boşluk karakterleri eklenir.\n    ----------------\n    \"\"\"\n    df = pd.DataFrame({\n        \"eval_row\": row,\n        \"document\": document,\n        \"token\": token,\n        \"label\": label,\n        \"token_str\": token_str\n    })\n\n    df = df.drop_duplicates().reset_index(drop=True)\n\n    df[\"row_id\"] = list(range(len(df)))\n    return df\n\"\"\"\n------------------------------\n+ triplets lsitesindeki sonuçlar pandas df'ine dönüştürülmüş\n+ dönüştürülürken tekrar eden yani duplikat satırlar kaldırılmış ve indeksler sıfırlanmış\n+ \"row_id\" satırı oluşturulmuş. her bir satırın benzersiz kimliğini ifade ediyor\n+ row_id eklenmiş bir şekilde df return edilir\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_df = parse_predictions(preds.predictions, id2label, valid_ds, threshold=threshold)\n\"\"\"\n---------------------------------\n- \"parse_predictions\" fonksiyonu kullanılarak modelin tahminleri \"preds_df\" değişkenine işlenmiş \n---------------------------------\n\"\"\"\n\n\n# modelin tahminlerinin çıktısı:\n# print(preds_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)\n\"\"\"\n---------------------------------\n- \"preds_df\" değişkenine işlediğimiz verilerin sadece \"row_id\", \"document\", \"token\", \"label\" sütunları alınarak\n  CSV dosyasına dönüştürülmüş\n  \n+ \"to_csv\" yöntemi ile bu df \"submission.csv\" dosyasına yazdırılmış\n---------------------------------\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}